# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KGwaFhwVMEp59mXCyAHrpEm5OLIbuGKQ
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# !pip install pandas
# !pip install matplotlib
# !pip install seaborn
# !pip install nltk
# !pip install statsmodels
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
import warnings
# %matplotlib inline
import statsmodels.api as sm
warnings.filterwarnings('ignore')

df = pd.read_csv('data.csv')
#df.head()
"""In this data set there are 3 attributes and they are label,id,tweet""", \
     """here our aim is to predict the label based on the tweet"""

def remove_pattern(input_txt, pattern):
    """This function will take inputtext and pattern and remove pattern form hte input text"""
    r = re.findall(pattern, input_txt)
    for word in r:
        input_txt = re.sub(word, "", input_txt)
    return input_txt

# remove twitter handles (@user)
"""removing unwanted data"""
df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], "@[\w]*")

df['clean_tweet'] = df['clean_tweet'].str.replace("[^a-zA-Z#]", " ")
#df.head()

# remove short words
"""removing short words whose lentght is less thab 3 characters"""
df['clean_tweet'] = df['clean_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
#df.head()

"""individual words considered as tokens"""
tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())
#tokenized_tweet.head()

# stem the words
"""stem the words"""
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])
#tokenized_tweet.head()

# combine words into single sentence
"""combine words into single sentence"""
for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = " ".join(tokenized_tweet[i])
    
df['clean_tweet'] = tokenized_tweet
#df.head()

def hashtag_extract(tweets):
    hashtags = []
    """DOc"""
    # loop words in the tweet
    for tweet in tweets:
        ht = re.findall(r"#(\w+)", tweet)
        hashtags.append(ht)
    return hashtags

ht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])
# extract hashtags from racist/sexist tweets
ht_negative = hashtag_extract(df['clean_tweet'][df['label']==1])
#ht_positive[:5]

ht_positive = sum(ht_positive, [])
ht_negative = sum(ht_negative, [])

freq = nltk.FreqDist(ht_positive)
d = pd.DataFrame({'Hashtag': list(freq.keys()),
                 'Count': list(freq.values())})
#d.head()

freq = nltk.FreqDist(ht_negative)
d = pd.DataFrame({'Hashtag': list(freq.keys()),
                 'Count': list(freq.values())})
#d.head()

from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
bow = bow_vectorizer.fit_transform(df['clean_tweet'])
bow = bow.toarray()
#bow

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(bow, df['label'], random_state=42, test_size=0.25)
#x_train

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier(n_neighbors=7)
  
knn.fit(x_train, y_train)
pred = knn.predict(x_test)

accuracy_score(y_test,pred)

#Importing the required modules
import numpy as np
from scipy.stats import mode
from scipy.spatial import distance
#     dist = np.sqrt(np.sum((p1-p2)**2))
    
    # return dist
#Euclidean Distance
def eucledian(p1,p2):
    """Docstring purpose"""
    return distance.euclidean(p1,p2)

 
def predict(x_train, y , x_input, k):
    """important function"""
    op_labels = []
     
    #Loop through the Datapoints to be classified
    for item in x_input:
        #Array to store distances
        point_dist = []
         
        #Loop through each training Data
        for j in range(x_train.shape[0]): 
            distances = eucledian(x_train[j,:] , item) 
            #Calculating the distance
            point_dist.append(distances) 
        point_dist = np.array(point_dist) 
         
        #Sorting the array while preserving the index
        #Keeping the first K datapoints
        dist = np.argsort(point_dist)[:k] 
         
        #Labels of the K datapoints from above
        labels = y[12]
         
        #Majority voting
        lab = mode(labels) 
        lab = lab.mode[0]
        op_labels.append(lab)
 
    return op_labels

#y_pred = predict(x_train,y_train,x_test, 7)